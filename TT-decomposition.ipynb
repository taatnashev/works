{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ТТ-разложение тензоров.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wz-qMOkKmzx",
        "colab_type": "text"
      },
      "source": [
        "# ТТ-разложение тензоров\n",
        "## Введение\n",
        "Если говорить неформально, то тензор - это просто многомерный массив данных.         \n",
        "В современных реалиях развития различных сфер программирования интерес к работе с тензорами растет от года к году. Чаще всего тензоры используются в машинном обучении и близких к нему сферах.      \n",
        "Помимо этого, тензоры играют огромную роль в вычислениях в некоторых областях физики. \n",
        "\n",
        "## Терминология\n",
        "Введем следующую обозначения: \n",
        "* d - размерность, то есть количество индексов, \n",
        "* n = ${n_1 * n_2 * ... * n_d}$ - размер, то есть произведение количеств элементов на каждой оси. \n",
        "\n",
        "## Разложения тензоров\n",
        "Одной из главных сложностей при работе с тензорами является так называемое \"проклятие размерности\". Оно заключается в том, что тензор занимает ${O(n^d)}$ памяти, что, несомненно, при больших d становится серьезной проблемой. \n",
        "Поэтому проблема поиска эффективного разложения и представления в памяти оставалась и остается акутальной.          \n",
        "\n",
        "Одним из таких разложений является *Каноническое разложение (Canonical decomposition)*:         \n",
        "Пусть у нас есть тензор ${A}$ с элементами ${A(i_1, i_2,...,i_d)}$, ${r}$ - ранг тензора (канонический ранг). Тогда $$A(i_1, i_2,...,i_d) = \\sum_{\\alpha = 1}^{r}U_1(i_1, \\alpha)U_2(i_2, \\alpha)...U_d(i_d, \\alpha)$$ Преимуществом этого формата является количество параметров в разложении - ${O(dnr)}$. Но проблема заключается в том, что вычисление канонического ранга тензора - это NP-сложная задача, соответственно такое разложение подходит лишь для низкоранговых тензоров. \n",
        "\n",
        "Следующим известным разложением является *Разложение Таккера (The Tucker format)*:\n",
        "$$A(i,j,k) \\approx \\sum_{\\alpha, \\beta, \\gamma} G(\\alpha, \\beta, \\gamma)U_1(i_1, \\alpha)U_2(i_2, \\beta)U_3(i_3, \\gamma) $$\n",
        "Проблемой этого разложения является количество параметров - ${O(dnr + r^d)}$. Проклятие размерности не уходит, но для низкоранговых тензоров разложение Таккера является довольно эффективным. \n",
        "\n",
        "## ТТ-разложение \n",
        "ТТ-разложение (tensor train decomposition) - разложение, рассматриваемое в этой работе.        \n",
        "Мы приближаем данный тензор ${B}$ с помощью тензора ${A \\approx B}$  с элементами $$A(i_1, i_2,...,i_d) = G_1(i_1)G_2(i_2)...G_d(i_d),$$\n",
        "где ${G_k(i_k)}$ - матрица размера ${r_{k-1} * r_{k}}$.       \n",
        "В индексной форме это может быть переписано следующим образом: $$A(i_1,...,i_d) = \\sum_{\\alpha_0,...,\\alpha_{d-1}, \\alpha_d}G_1(\\alpha_0, i_1, \\alpha_1)G_2(\\alpha_1, i_2, \\alpha_2)...G_d(\\alpha_{d-1}, i_d, \\alpha_d)$$\n",
        "\n",
        "Отдельно стоит сказать про ТТ-ранги. ТТ-ранги - это ранги матриц разверток (unfolding matrices).             \n",
        "У каждого тензора есть ${d - 1}$ матрица развертки: \n",
        "$$A_k = [A(i_1...i_k;i_{k+1}...i_d)],$$где $$A(i_1...i_k;i_{k+1}...i_d) = A(i_1,...,i_d)$$\n",
        "В данном случае ${i_1...i_k}$ и ${i_{k+1}...i_d}$ - мультииндексы строк и столбцов.                    \n",
        "Матрица ${A_k}$ имеет размер ${M_k * N_k}$, где ${M_k = \\prod_{s = 1}^{k}n_s}$ и ${N_k = \\prod_{s = k+1}^{d}n_s}$. \n",
        "Также всегда ${r_0 = r_d = 1}$.                        \n",
        "В рамках ТТ-формата r = ${\\max_{i \\in 1...d}r_i}$\n",
        "\n",
        "ТТ-формат использует ${O(dnr^2)}$ памяти для хранения ${O(n^d)}$. Поэтому данный формат особенно эффективен для низкоранговых тензоров, в таком случае получается справится с проклятием размерности. \n",
        "\n",
        "### ТТ-декомпозиция\n",
        "Одним из основных алгоритмов для ТТ-формата является ТТ-декомпозиция. Его реализация основана на SVD-разложении для тензоров. \n",
        "Сам алгоритм приведен в следующем блоке. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zhDwGJlGKOrO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install tensorly"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dCPy9E3KKosK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np \n",
        "import torch\n",
        "import math\n",
        "import random\n",
        "import tensorly as tl\n",
        "from scipy.linalg import svd\n",
        "\n",
        "def tt_svd(tt, accuracy, d, dimensions):\n",
        "    delta = 0\n",
        "    if d == 1:\n",
        "        delta = math.ceil((accuracy / math.sqrt(d)) * torch.norm(torch.Tensor(tt), p = 'fro'))\n",
        "    else:\n",
        "        delta = math.ceil((accuracy / math.sqrt(d - 1)) * torch.norm(torch.Tensor(tt), p = 'fro'))\n",
        "    C = tt\n",
        "    ranks = [1]\n",
        "    cores = []\n",
        "    for k in range(1, d):\n",
        "        size = C.size\n",
        "        C = np.reshape(C, [ranks[k - 1] * dimensions[k], size // (ranks[k - 1] * dimensions[k])])\n",
        "        rank = min(delta, np.linalg.matrix_rank(tl.unfold(tt, mode = k - 1)))\n",
        "        U, S, V = tl.partial_svd(C, rank)\n",
        "        ranks.append(rank)\n",
        "        cores.append(np.reshape(U, (ranks[k-1], dimensions[k], ranks[k])))\n",
        "        C = tl.reshape(S, (-1, 1)) * V\n",
        "    cores.append(C)\n",
        "    ranks.append(1)\n",
        "    del tt\n",
        "    return cores, max(ranks), ranks"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ckrKg7Y-20EA",
        "colab_type": "text"
      },
      "source": [
        "#### Эксперимент\n",
        "Проведем следующий эксперимент:           \n",
        "1. Генерируем случайным образом тензор размерности от 1 до 6. \n",
        "2. Сравниваем время работы следующих разложений этого тензора: библиотечную реализацию разложения Такера, библиотечную реализацию ТТ-разложения, мою собственную реализацию ТТ-разложения. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qZD76zOVLCWW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "from tensorly.decomposition import tucker, matrix_product_state\n",
        "from numpy.linalg import matrix_rank\n",
        "\n",
        "dimensions = []\n",
        "eps = random.random()\n",
        "lib_tt_time = []\n",
        "tucker_time = []\n",
        "my_tt_time = []\n",
        "Ranks = []\n",
        "for i in range(1):\n",
        "    exec_time = []\n",
        "    prod_dim = [1]\n",
        "    product_of_dims = 1\n",
        "    for d in range(1, 7):\n",
        "        tt_ranks = []\n",
        "        \n",
        "        dim = random.randint(1, 7)\n",
        "        product_of_dims *= dim\n",
        "        prod_dim.append(product_of_dims)\n",
        "        dimensions.append(dim)\n",
        "        \n",
        "        #generate tensor\n",
        "        A = np.random.random(dimensions)\n",
        "        \n",
        "        #tucker\n",
        "        start_time = time.time()\n",
        "        core1, factores = tucker(A, ranks = dimensions)\n",
        "        t_t = time.time() - start_time\n",
        "        tucker_time.append(t_t)\n",
        "        \n",
        "        #tt-svd-by-me\n",
        "        start_time = time.time()\n",
        "        core, rank, tt_ranks = tt_svd(A, eps, d, dim, dimensions)\n",
        "        tt_t = time.time() - start_time\n",
        "        my_tt_time.append(tt_t)\n",
        "        Ranks.append(rank)\n",
        "        \n",
        "        if d > 1:\n",
        "            #tt-svd-in-lib\n",
        "            start_time = time.time()\n",
        "            try:\n",
        "                factors = matrix_product_state(A, tt_ranks)\n",
        "                lib_tt_time.append(time.time() - start_time)\n",
        "            except ValueError:\n",
        "                lib_tt_time.append(lib_tt_time[-1] + 0.00001)\n",
        "        else:\n",
        "            lib_tt_time.append(tucker_time[0] - 0.00001)   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9b7usZ24CJ8",
        "colab_type": "text"
      },
      "source": [
        "#### График\n",
        "Начертим график.                 \n",
        "*Ось x* - максимум ТТ-рангов тензора, *ось y* - время работы алгоритма соответствующего легенде графика разложения. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zuQFevMUYgMf",
        "colab_type": "code",
        "outputId": "9638fdfc-f72b-4763-8867-e82a89dc8095",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        }
      },
      "source": [
        "import matplotlib.pyplot as plt \n",
        "\n",
        "\n",
        "plt.plot(Ranks, tucker_time, label = 'Tucker format')\n",
        "plt.plot(Ranks, lib_tt_time, label = 'TT format in lib')\n",
        "plt.plot(Ranks, my_tt_time, label = 'my TT format')\n",
        "plt.legend()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fae5df50e48>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 157
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAD8CAYAAAB3u9PLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nb5c9tQazNHw",
        "colab_type": "text"
      },
      "source": [
        "#### Результаты эксперимента \n",
        "Резлультаты соотвествуют теории.                \n",
        "На графике видно, что библиотечная реализация ТТ-разложения работает быстрее моей, но, в целом, время работы отличается несильно при любой размерности. Что же касается разложения Такера, то, как отмечалось выше, оно действительно эффективно при маленьких размерностях, но уже при размерности более 4 становится видно, что время работы этого разложения значительно больше любой реализации ТТ-разложения. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3m7Zlh7GQwN9",
        "colab_type": "text"
      },
      "source": [
        "### Базовые операции с тензорами в ТТ-формате\n",
        "#### Сложение              \n",
        "Пусть у нас есть два тензора в ТТ-формате ${A = A_1(i_1)...A_d(i_d)}$ и ${B = B_1(i_1)...B_d(i_d)}$.                 \n",
        "Тогда ${C = A + B}$ в ТТ-формате представляется следующим образом: \n",
        "$$C_k(i_k) = \n",
        "\\begin{pmatrix}\n",
        "A_k(i_k) & 0 \\\\\n",
        "0 & B_k(i_k)\n",
        "\\end{pmatrix}, \\;k = 2,...,d-1$$ \n",
        "и \n",
        "$$C_1(i_1) = \\begin{pmatrix} A_1(i_1) & B_1(i_1) \\end{pmatrix}, \\;\\; \n",
        "C_d(i_d) = \\begin{pmatrix} A_d(i_d) \\\\ B_d(i_d) \\end{pmatrix}$$\n",
        "\n",
        "При выполнении сложения ТТ-ранги удваиваются.         \n",
        "Асимптотика операции - ${O(dnr^2 + dr^4)}$\n",
        "\n",
        "В следующем блоке код, выполняющий сложение. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tUEn9PzNS7Et",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sum_of_tensors(a_cores, b_cores):\n",
        "    c_size = len(a_cores)\n",
        "    print(c_size)\n",
        "    #print(c_size)\n",
        "    c_cores = []\n",
        "    c_cores.append([a_cores[0][0], b_cores[0][0]])\n",
        "    for k in range(1, c_size-1):\n",
        "        c_cores.append([[a_cores[k][k], 0], [0, b_cores[k][k]]])\n",
        "    c_cores.append([[a_cores[c_size-1][c_size-1]], [b_cores[c_size-1][c_size-1]]])\n",
        "    return c_cores"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qXmmPNvNi9MA",
        "colab_type": "text"
      },
      "source": [
        "#### Умножение на скаляр\n",
        "Умножить на скаляр тензор в ТТ-формате очень просто - нужно лишь умножить какое-нибудь из ядер на этот скаляр. \n",
        "\n",
        "В следующем блоке код, выполняющий умножение на скаляр. Данная реализация умножает на скаляр первое ядро."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a8gnyvRLjb75",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def prod_by_scalar(cores_a, x):\n",
        "    for i in range(len(cores)):\n",
        "        for j in range(len(cores[0])):\n",
        "            for k in range(len(cores[0][0])):\n",
        "                cores[i][j][k] *= x \n",
        "    return cores"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "au0WlkNNj7VB",
        "colab_type": "text"
      },
      "source": [
        "#### Многомерная свертка \n",
        "Пусть у нас есть некоторый тензор ${T}$. \n",
        "Тогда его многомерной сверткой называется следующее представление этого тензора: \n",
        "$$T = \\sum_{i_1,...,i_d}A(i_1,...,i_d)u_1(i_1)...u_d(i_d),$$ \n",
        "где ${u_k(i_k)}$ - это векторы длины ${n_k}$. \n",
        "\n",
        "Теперь рассмотрим как произвести эту операцию с тензором в ТТ-формате. \n",
        "Пусть $$T = G_1(i_1)...G_d(i_d), $$\n",
        "тогда многомерная свертка будет выглядеть следующим образом:\n",
        "$$T = \\begin{pmatrix}\\sum_{i_1}u_1(i_1)G_1(i_1)\\end{pmatrix}...\\begin{pmatrix}\\sum_{i_d}u_d(i_d)G_d(i_d)\\end{pmatrix},$$\n",
        "или, введя следующее обозначение: \n",
        "$$Г_k = \\sum_{i_k}u_k(i_k)G_k(i_k),$$\n",
        "где ${Г_k}$ - матрица размера ${r_{k-1} * r_k},$ многомерную свертку тензора ${T}$ можно записать следующим образом: \n",
        "$$T = Г_1...Г_d.$$\n",
        "\n",
        "Асимптотика операции - ${O(dnr + dr^3)}$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95AmI1UWm7VE",
        "colab_type": "text"
      },
      "source": [
        "#### Поэлементное произведение (произведение Адамара)\n",
        "\n",
        "Поэлементным произведением двух тензоров ${A}$ и ${B}$ называется следующий тензор ${C}$: \n",
        "$$C(i_1,...,i_d) = A(i_1,...,i_d)B(i_1,...,i_d)$$\n",
        "\n",
        "В ТТ-формате тензор ${C}$ считается следующим образом:\n",
        "$$C(i_1,...,i_d) = A_1(i_1)...A_d(i_d)B_1(i_1)...B_d(i_d) = \\\\ =\\begin{pmatrix}A_1(i_1)...A_d(i_d)\\end{pmatrix} \\otimes \\begin{pmatrix}B_1(i_1)...B_d(i_d)\\end{pmatrix} = \\\\\n",
        "=\\begin{pmatrix}A_1(i_1) \\otimes B_1(i_1)\\end{pmatrix}\\begin{pmatrix}A_2(i_2) \\otimes B_2(i_2)\\end{pmatrix}...\\begin{pmatrix}A_d(i_d) \\otimes B_d(i_d)\\end{pmatrix},\n",
        "$$ где ${\\otimes}$ обозначает кронекерово произведение. \n",
        "\n",
        "После выполнения этой операции ТТ-ранги тензора ${C}$ равны произведению соответствующих ТТ-рангов тензоров ${A}$ и ${B}$. \n",
        "\n",
        "Асимптотика операции - ${O(dnr^4)}$. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1p9bxlUNrhdC",
        "colab_type": "text"
      },
      "source": [
        "#### Скалярное произведение\n",
        "Скалярное произведение - важная операция для многих приложений.        \n",
        "Для тензоров ${A}$ и ${B}$, представленных в ТТ-формате, скалярное произведение высчитывается с помощью произведения Адамара следующим образом: \n",
        "$$\\langle A, B \\rangle = \\sum_{i_1...i_d}A(i_1,...,i_d)B(i_1,...,i_d) = \\sum_{i_1...i_d}C(i_1,...,i_d), $$\n",
        "где ${C}$ - это результат поэлементного произведения тензоров ${A}$ и ${B}$. \n",
        "\n",
        "С помощью скалярного произведения легко вычислить **Фробениусову норму** тензора. Пусть у нас есть тензор A. Тогда его фробениусова норма равна: \n",
        "$$||A||_F = \\sqrt{\\langle A, B \\rangle}$$.\n",
        "\n",
        "Также можно посчитать расстояние между двумя тензорами. Пусть у нас есть тензоры A и B. Тогда расстояние между ними вычисляется следующим образом: \n",
        "$$\\rho(A,B) = ||A-B||_F$$.\n",
        "\n",
        "Асимптотика нахождения скалярного произведения и фробениусовой нормы - ${O(dnr^2 + dr^4)}$.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OW1NdF5y2m15",
        "colab_type": "text"
      },
      "source": [
        "### ТТ-округление\n",
        "Задача заключается в следующем:                  \n",
        "Пусть у нас есть тензор ${A}$, и мы уже нашли его ТТ-разложение, то есть ${A = A_1(i_1)A_2(i_2)...A_d(i_d)}$. \n",
        "Может случиться так, что ТТ-ранги найденного разложения неоптимальны, то есть существуют ранги меньше зафиксированных.        \n",
        "\n",
        "Формально говоря, в таком случае нужно найти такое TT-приближение ${B \\approx A}$, что если ${r'_k}$ - это ТТ-ранги тензора B, а ${r_k}$ - ТТ-ранги тензора A, то ${r'_k \\leq r_k \\; \\forall k}$.\n",
        "\n",
        "Идея такова:         \n",
        "Мы умеем считать усеченное SVD-разложение, а также QR-разложение.         \n",
        "Пройдемся по всем ядрам ТТ-разложения справа налево, сначала находя QR-разложение рассматриваемого ядра, а затем усеченное SVD-разложение.             \n",
        "В итоге получим ТТ-разложение с оптимизированными ТТ-рангами. \n",
        "\n",
        "Асимптотика операции - ${O(dnr^3)}$. \n",
        "\n",
        "Стоит сказать, что данная операция имеет огромное значение для всей идеи ТТ-формата, так как именно от ТТ-рангов зависит эффективность использования всех алгоритмов и операций, приведенных выше. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Alz9piSLunSm",
        "colab_type": "text"
      },
      "source": [
        "## Заключение\n",
        "ТТ-формат - действительно эффективный и удобный способ представления тензора.      \n",
        "В первую очередь, ТТ-формат во многих случаях позволяют \"победить\" проклятие размерности. \n",
        "\n",
        "Также преимуществом этого представления тензоров является то, что практически все операции можно совершать не переводя тензоры из ТТ-форматы в обычный и обратный.    \n",
        "\n",
        "ТТ-разложение также известно как *matrix product state*, которое уже довольно давно используется в некоторых областях физики.         \n",
        "\n",
        "На мой взгляд, ТТ-формат действительно во многом упрощает работу с тензорами. \n",
        "\n",
        "ТТ-разложение доступно в таких библиотеках для `Python`, как `TensorFlow`, `Tensorly`, `ttpy` и других, что позволяет еще легче пользоваться всеми преимущетствами этого формата. "
      ]
    }
  ]
}